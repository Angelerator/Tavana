# Tavana Helm Chart Values - Community Edition
# Cloud-Agnostic Auto-Scaling DuckDB Query Platform

# Global settings
global:
  namespace: tavana
  # Container registry - GHCR for public, ACR for Azure (see values-azure.yaml)
  # NOTE: Some AKS clusters block external registries. Use ACR in those cases.
  imageRegistry: "ghcr.io/angelerator"
  imagePullPolicy: IfNotPresent
  imageTag: "latest"

# Image pull secrets (for private registries)
imagePullSecrets: []
  # - name: acr-secret
  # - name: dockerhub-secret

# Gateway settings
gateway:
  enabled: true
  replicaCount: 2
  image:
    repository: "tavana-gateway"
    tag: ""  # Defaults to global.imageTag
  
  # K8s 1.35+: In-place resource updates (VPA can resize without pod restart)
  # Enable this after upgrading to K8s 1.35+
  resizePolicy:
    enabled: false      # Set to true on K8s 1.35+
    cpu: "NotRequired"  # CPU can resize without restart
    memory: "RestartContainer"  # Memory resize requires container restart
  
  service:
    type: ClusterIP
    pgPort: 5432       # PostgreSQL wire protocol (internal)
    httpPort: 8080     # Health + Metrics
  
  # External LoadBalancer for client access (Tableau, PowerBI, psql)
  loadBalancer:
    enabled: false              # Enable to expose externally
    externalPort: 443           # External port (443 passes corporate firewalls)
    # loadBalancerIP: ""        # Optional: static IP
    # loadBalancerSourceRanges: # Optional: IP allowlist
    #   - "10.0.0.0/8"
    exposeHttp: false           # Expose HTTP health/metrics externally
    httpExternalPort: 8080
    exposeFlight: true          # Expose Arrow Flight SQL externally (2.2x faster)
    flightExternalPort: 50051   # Standard gRPC port
    annotations: {}
    # Azure-specific annotations:
    #   service.beta.kubernetes.io/azure-load-balancer-internal: "true"
    #   service.beta.kubernetes.io/azure-load-balancer-internal-subnet: "subnet-name"
  
  # Arrow Flight SQL configuration (ADBC-compatible, high-performance binary protocol)
  # Enables Arrow-native database connectivity for analytics clients
  # Supported clients:
  #   - Python: adbc_driver_flightsql, pyarrow.flight
  #   - Go: github.com/apache/arrow-adbc/go/adbc/driver/flightsql
  #   - Java: org.apache.arrow.adbc:adbc-driver-flight-sql
  #   - JDBC: jdbc:arrow-flight-sql://host:port
  flightSql:
    enabled: true
    port: 50051        # Arrow Flight gRPC port (standard gRPC port)
  
  # TLS configuration for PostgreSQL wire protocol
  tls:
    # Enable TLS/SSL support (accepts both SSL and non-SSL connections)
    enabled: false
    # Self-signed certificate generation (for development/testing)
    selfSigned: true
    # Common name for self-signed certificate
    commonName: "tavana.local"
    # Use existing TLS secret instead of self-signed
    existingSecret: ""
    # Keys in the existing secret (PEM format)
    existingSecretCertKey: "tls.crt"
    existingSecretKeyKey: "tls.key"
  
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  
  # gRPC Performance Tuning (Gateway → Worker communication)
  # These settings control HTTP/2 flow control for Arrow IPC streaming.
  # Higher values reduce WINDOW_UPDATE frame overhead for large result sets.
  # Workers use 512MB/1GB by default — gateway should match for optimal throughput.
  grpc:
    # HTTP/2 stream window size (per-stream flow control)
    # Higher = fewer flow control stalls, but more memory per stream
    streamWindowMB: 512       # Default: 2MB → 512MB (256x increase)
    # HTTP/2 connection window size (aggregate across all streams)
    connectionWindowMB: 1024  # Default: 4MB → 1GB (256x increase)
    # Internal channel buffer for streaming batches
    # Higher = more batches buffered before backpressure kicks in
    channelBuffer: 256        # Default: 32 → 256 (8x increase)
  
  # Environment variables
  env: []
  
  # Liveness and readiness probes
  livenessProbe:
    enabled: true
    path: /health
    initialDelaySeconds: 30
    periodSeconds: 10
  readinessProbe:
    enabled: true
    path: /health
    initialDelaySeconds: 10
    periodSeconds: 5

# Worker settings (managed by gateway's smart scaler)
worker:
  enabled: true
  replicaCount: 2
  minReplicas: 2
  maxReplicas: 20
  image:
    repository: "tavana-worker"
    tag: ""  # Defaults to global.imageTag
  
  # K8s 1.35+: In-place resource updates (VPA can resize without pod restart)
  # Enable this after upgrading to K8s 1.35+
  resizePolicy:
    enabled: false      # Set to true on K8s 1.35+
    cpu: "NotRequired"  # CPU can resize without restart
    memory: "RestartContainer"  # Memory resize may require container restart
  
  # K8s 1.35+: Fine-grained restart rules for specific exit codes
  # Enable this after upgrading to K8s 1.35+
  restartPolicy:
    # Uncomment to enable on K8s 1.35+:
    # rules:
    #   - exitCodes: [137]     # OOMKilled
    #     action: Restart
    #   - exitCodes: [143]     # SIGTERM
    #     action: Restart
    #   - exitCodes: [1]       # General error
    #     action: Restart
    #     maxRestarts: 3
    rules: []
  
  resources:
    requests:
      memory: "4Gi"
      cpu: "2"
    limits:
      memory: "16Gi"
      cpu: "4"
  
  # Environment variables
  env: []
  
  # Node selector for dedicated worker nodes
  nodeSelector: {}
    # nodepool: tavana
  
  # Tolerations for dedicated worker nodes
  tolerations: []
    # - key: "workload"
    #   operator: "Equal"
    #   value: "tavana"
    #   effect: "NoSchedule"

# Horizontal Pod Autoscaler
hpa:
  enabled: true
  minReplicas: 2
  maxReplicas: 20
  cpuTargetUtilization: 70
  memoryTargetUtilization: 80
  # Custom metrics (requires metrics-server and prometheus-adapter)
  queueDepthTarget: "5"
  queueWaitTimeSecondsTarget: "30s"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      # K8s 1.35+: Custom tolerance for more responsive scaling
      # Default is 0.1 (10%). Set to 0.05 for 5% sensitivity.
      # Uncomment after upgrading to K8s 1.35+:
      # tolerance: 0.05
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      # K8s 1.35+: Custom tolerance for scale-down decisions
      # Uncomment after upgrading to K8s 1.35+:
      # tolerance: 0.1
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120

# Vertical Pod Autoscaler
vpa:
  enabled: true
  gateway:
    updateMode: "Auto"
    minCpu: "100m"
    minMemory: "512Mi"
    maxCpu: "4"
    maxMemory: "8Gi"
  worker:
    updateMode: "Auto"
    minCpu: "250m"
    minMemory: "2Gi"
    maxCpu: "8"
    maxMemory: "32Gi"

# Pod Disruption Budgets
pdb:
  enabled: true
  gateway:
    minAvailable: 1
  worker:
    minAvailable: 2

# Object Storage (S3-compatible)
objectStorage:
  # Endpoint for S3-compatible storage
  endpoint: ""
  # Bucket name for data
  bucket: ""
  # Region (for AWS S3)
  region: "us-east-1"
  # Use path-style access (required for MinIO, ADLS with S3 API)
  pathStyle: true
  # Credentials (use existingSecret in production)
  accessKeyId: ""
  secretAccessKey: ""
  existingSecret: ""
  existingSecretAccessKeyIdKey: "access-key-id"
  existingSecretSecretAccessKeyKey: "secret-access-key"

# Service Account
serviceAccount:
  create: true
  name: "tavana"
  annotations: {}
  # AWS IRSA annotation:
  # eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT:role/tavana-role
  # Azure Workload Identity annotation:
  # azure.workload.identity/client-id: CLIENT_ID

# Pod Security
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault

containerSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL

# Network Policies
networkPolicies:
  enabled: true

# Ingress configuration
ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: tavana.example.com
      paths:
        - path: /
          pathType: Prefix
  tls: []

# Monitoring
monitoring:
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: false
      namespace: ""
      labels: {}

# =============================================================================
# Authentication Gateway Configuration
# =============================================================================
# Tavana supports multiple authentication modes and providers.
# Auth is OPTIONAL - community editions can run without any auth.
#
# Modes:
#   - passthrough: No auth required (default for community edition)
#   - required:    Auth required via configured providers
#   - optional:    Allow anonymous if no credentials provided
#
# Providers (can enable multiple):
#   - separ:       Centralized authorization platform (enterprise)
#   - jwt:         Direct JWT/OIDC validation (any IdP)
#   - staticKeys:  Simple API key authentication
#
auth:
  # Auth mode - "passthrough" for community/dev, "required" for enterprise
  mode: "passthrough"
  
  # --- Provider 1: Separ (Enterprise Authorization Platform) ---
  # Enable for centralized multi-tenant authorization
  separ:
    enabled: false
    endpoint: "http://separ.separ.svc.cluster.local:8080"
    enableAuthz: true
    tenantExtraction: "email_domain"  # email_domain, backslash_prefix, none
    existingSecret: ""                 # K8s secret with API key
    existingSecretKey: "separ-api-key"
    # Connection tuning
    connectTimeoutMs: 5000
    requestTimeoutMs: 10000
    poolIdleTimeoutSecs: 90
    poolMaxIdlePerHost: 10
  
  # --- Provider 2: Direct JWT/OIDC (Any Identity Provider) ---
  # Enable for direct JWT validation without Separ
  # Works with: Azure AD, Okta, Auth0, Keycloak, etc.
  jwt:
    enabled: false
    # issuer: "https://your-idp.example.com"
    # audience: "tavana"
    # algorithm: "RS256"  # RS256 for asymmetric, HS256 for symmetric
    # jwksUri: "https://your-idp.example.com/.well-known/jwks.json"
    # secretOrKeyPath: ""  # For HS256 or local public key
    # clockSkewSecs: 60
  
  # --- Provider 3: Static API Keys ---
  # Enable for simple service-to-service auth
  staticKeys:
    enabled: false
    # keys: []  # Define via secrets, not inline
  
  # --- Auth Caching ---
  cache:
    enabled: true
    ttlSecs: 300        # Cache successful auth for 5 minutes
    maxSize: 10000      # Max cached entries
  
  # --- Rate Limiting ---
  rateLimit:
    enabled: true
    maxFailedAttempts: 5
    banDurationSecs: 300

# ConfigMap for gateway configuration
config:
  # Log level: debug, info, warn, error
  logLevel: "info"
  # Worker discovery endpoint
  workerServiceName: "worker"
  workerServicePort: 50053

# RBAC for gateway to manage workers
rbac:
  create: true

# Pod Disruption Budgets
podDisruptionBudget:
  gateway:
    enabled: true
    minAvailable: 1
  worker:
    enabled: true
    maxUnavailable: 1

# =============================================================================
# Query Result Caching (Redis/Dragonfly)
# =============================================================================
# Distributed cache for query results to avoid re-executing identical queries.
# Supports Redis and Dragonfly (25x faster, drop-in compatible).
#
# Best practices applied:
#   - TTL-based expiration (configurable per query pattern)
#   - Two-tier caching: L1 (in-memory) + L2 (Redis/Dragonfly)
#   - LZ4 compression for large results
#   - BLAKE3 hashing for O(1) key lookups
#
queryCache:
  enabled: false  # Enable to use caching (requires Redis/Dragonfly)
  # Redis/Dragonfly connection URL
  # Examples:
  #   - redis://redis.tavana.svc:6379
  #   - redis://:password@redis-master:6379/0
  #   - rediss://secure-redis:6380 (TLS)
  redisUrl: "redis://redis:6379"
  # Default TTL for cached results (seconds)
  # Aggregate queries get 2x TTL, point queries get 0.5x TTL
  ttlSecs: 300
  # Maximum cacheable result size (MB)
  maxResultSizeMB: 100
  # Enable LZ4 compression for results > 1KB
  compression: true
  # Cache version (increment to invalidate all entries)
  version: 1
  # L1 (in-memory) cache settings
  l1:
    enabled: true
    size: 1000      # Number of entries
    ttlSecs: 30     # Local cache TTL
  # Use existing Redis secret for authentication
  existingSecret: ""
  existingSecretKey: "redis-password"

# =============================================================================
# Worker Remote File Caching (cache_httpfs)
# =============================================================================
# DuckDB community extension for caching remote HTTP/S3/Azure data blocks.
# Provides 11.6x speedup on repeated queries (documented benchmarks).
#
# How it works:
#   - Caches data blocks from remote files to local disk
#   - Supports parallel I/O with tunable parallelism
#   - Provides cache hit/miss profiling
#   - Acts as a drop-in replacement for httpfs
#
cacheHttpfs:
  enabled: true  # Enabled by default for performance
  # Cache directory (must be writable by worker)
  # For production: use a PVC-backed volume for persistence across restarts
  cacheDir: "/tmp/duckdb_cache"
  # Maximum disk cache size (GB)
  # Set based on available disk space on worker nodes
  maxSizeGB: 50
  # Block size for caching (MB)
  # Larger = fewer HTTP requests, more memory per block
  # 8MB is good for typical Parquet row groups
  blockSizeMB: 8
  # Number of parallel I/O requests
  # Higher = more throughput, more concurrent connections
  parallelIO: 8
  # Enable profiling (logs cache hit/miss statistics)
  profiling: false

# =============================================================================
# Network Performance Tuning (TCP BBR)
# =============================================================================
# DaemonSet that applies high-performance kernel network settings to all nodes.
# Based on ESnet (DOE) and Google research for high-bandwidth data transfer.
#
# TCP BBR benefits:
#   - 10x performance on paths with packet loss
#   - Avoids bufferbloat by not filling bottleneck buffers
#   - Better throughput for high-latency connections (cloud cross-region)
#
# References:
#   - https://fasterdata.es.net/host-tuning/linux
#   - https://cloud.google.com/blog/products/networking/tcp-bbr-congestion-control
#
networkTuning:
  enabled: false  # Enable to deploy network tuner DaemonSet
  # TCP congestion control algorithm
  # Options: bbr (recommended), cubic (default), reno
  # BBR requires Linux kernel 4.9+ (all modern K8s nodes have this)
  tcpCongestionControl: "bbr"
  # Maximum TCP buffer sizes (MB) for 10G+ networks
  # ESnet recommends 32-64MB per socket for high-bandwidth paths
  tcpRmemMaxMB: 64
  tcpWmemMaxMB: 64
  # Socket backlog settings for high-connection scenarios
  somaxconn: 65535
  netdevMaxBacklog: 65535
  tcpMaxSynBacklog: 65535
  # TCP Fast Open (0=disabled, 1=client, 2=server, 3=both)
  tcpFastOpen: 3
  # MTU probing (0=disabled, 1=enabled when ICMP blackhole detected, 2=always)
  tcpMtuProbing: 1
  # Reuse TIME_WAIT sockets (safe for most workloads)
  tcpTwReuse: true
  # TIME_WAIT timeout (seconds, default 60 is often too long)
  tcpFinTimeout: 15
  # TCP keepalive settings (faster dead connection detection)
  tcpKeepaliveTime: 60    # Start probes after 60s idle
  tcpKeepaliveIntvl: 10   # Probe interval 10s
  tcpKeepaliveProbes: 6   # 6 probes before declaring dead
  # Node selector (optional, limit to specific node pools)
  nodeSelector: {}
    # nodepool: tavana
