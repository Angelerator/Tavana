# Tavana Helm Chart - Azure Values Template
# Cloud-Agnostic Auto-Scaling DuckDB Query Platform
#
# Usage:
#   helm install tavana ./helm/tavana -f values-azure.yaml \
#     --set global.imageRegistry=YOUR_ACR.azurecr.io \
#     --set ingress.hosts[0].host=tavana.example.com

# Global settings
global:
  namespace: tavana
  # Container registry - Override with your ACR at deploy time:
  #   helm install tavana ./helm/tavana -f values-azure.yaml \
  #     --set global.imageRegistry=YOUR_ACR.azurecr.io \
  #     --set global.imageTag=1.0.73
  # 
  # NOTE: Many enterprise AKS clusters block external registries (GHCR, Docker Hub)
  # due to firewall rules. Always use your private ACR in those cases.
  imageRegistry: ""  # REQUIRED: Set via --set global.imageRegistry=YOUR_ACR.azurecr.io
  imagePullPolicy: Always
  imageTag: ""       # REQUIRED: Set via --set global.imageTag=VERSION

# Private registry credentials (optional for Docker Hub public, required for ACR)
imagePullSecrets: []
  # - name: acr-secret

# ═══════════════════════════════════════════════════════════════════════════════
# GATEWAY CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════
gateway:
  enabled: true
  replicaCount: 2
  image:
    repository: tavana-gateway
  
  # K8s 1.35+: In-place resource updates (VPA can resize without pod restart)
  # Enable this after upgrading to K8s 1.35+ (GA in March 2026)
  resizePolicy:
    enabled: false      # Set to true on K8s 1.35+
    cpu: "NotRequired"  # CPU can resize without restart
    memory: "RestartContainer"  # Memory resize requires container restart
  
  service:
    type: ClusterIP
    pgPort: 15432
    httpPort: 8080
  
  # External LoadBalancer for client access (Tableau, PowerBI, psql)
  loadBalancer:
    enabled: true               # Expose Tavana externally
    externalPort: 443           # External port (443 passes corporate firewalls)
    exposeHttp: false
    exposeFlight: true          # Expose Arrow Flight SQL (2.2x faster than PG wire)
    flightExternalPort: 443     # Use 443 for gRPC — best corporate network compatibility
    annotations:
      # Use internal Azure Load Balancer (recommended for security)
      # service.beta.kubernetes.io/azure-load-balancer-internal: "true"
  
  # Arrow Flight SQL configuration (ADBC-compatible, 2.2x faster than PG wire)
  # NOTE: Use port 443 externally for best Azure network/firewall compatibility.
  # Non-standard ports (9091, 50051) may fail on Azure Internal LB with HTTP/2.
  flightSql:
    enabled: true
    port: 9091               # Internal gRPC port for Arrow Flight SQL
  
  # Resource sizing — gateway serializes Arrow batches to PG wire / Flight SQL
  # More CPU = faster serialization for large result sets
  resources:
    requests:
      memory: "2Gi"
      cpu: "1"
    limits:
      memory: "8Gi"
      cpu: "4"
  
  # gRPC Performance Tuning (Gateway → Worker communication)
  # High-performance settings for Azure deployments — matches worker HTTP/2 config.
  # These eliminate flow control backpressure on large analytical result sets.
  grpc:
    streamWindowMB: 512       # HTTP/2 per-stream window (2MB → 512MB)
    connectionWindowMB: 1024  # HTTP/2 connection window (4MB → 1GB)
    channelBuffer: 256        # Streaming channel buffer (32 → 256 batches)

# ═══════════════════════════════════════════════════════════════════════════════
# WORKER CONFIGURATION (Query Execution)
# ═══════════════════════════════════════════════════════════════════════════════
worker:
  enabled: true
  replicaCount: 2
  minReplicas: 2
  maxReplicas: 8
  image:
    repository: tavana-worker
  
  # K8s 1.35+: In-place resource updates (VPA can resize without pod restart)
  # Enable this after upgrading to K8s 1.35+ (GA in March 2026)
  resizePolicy:
    enabled: false      # Set to true on K8s 1.35+
    cpu: "NotRequired"  # CPU can resize without restart
    memory: "RestartContainer"  # Memory resize may require container restart
  
  # K8s 1.35+: Fine-grained restart rules for specific exit codes
  # Enable after upgrading to K8s 1.35+ for automatic retry on OOM/transient errors
  restartPolicy:
    # Uncomment to enable on K8s 1.35+:
    # rules:
    #   - exitCodes: [137]     # OOMKilled - auto-retry
    #     action: Restart
    #   - exitCodes: [143]     # SIGTERM - graceful shutdown
    #     action: Restart
    #   - exitCodes: [1]       # General error - retry with limit
    #     action: Restart
    #     maxRestarts: 3
    rules: []
  
  # Resource sizing for high-throughput DuckDB execution
  # DuckDB uses 1 thread per CPU core — more cores = faster scans
  # Memory determines whether large scans stay in-memory or spill to disk
  resources:
    requests:
      memory: "4Gi"
      cpu: "2"
    limits:
      memory: "16Gi"       # DuckDB can use up to 80% for query execution
      cpu: "4"             # 4 cores = 4 DuckDB threads per worker

# ═══════════════════════════════════════════════════════════════════════════════
# AUTOSCALING (HPA)
# ═══════════════════════════════════════════════════════════════════════════════
hpa:
  enabled: true
  minReplicas: 2
  maxReplicas: 8           # Max 8 workers (fits 4 nodes)
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 75
  # HPA behavior settings
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      # K8s 1.35+: Custom tolerance for more responsive scaling
      # Default is 0.1 (10%). Set to 0.05 to scale at 5% change.
      # Uncomment after upgrading to K8s 1.35+:
      # tolerance: 0.05
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      # K8s 1.35+: Custom tolerance for scale-down decisions
      # Uncomment after upgrading to K8s 1.35+:
      # tolerance: 0.1
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120

# ═══════════════════════════════════════════════════════════════════════════════
# OBJECT STORAGE (Azure Blob / ADLS Gen2)
# ═══════════════════════════════════════════════════════════════════════════════
objectStorage:
  # Leave empty to use Workload Identity with Azure SDK
  endpoint: ""
  bucket: ""
  region: "westeurope"
  pathStyle: false
  existingSecret: ""

# ═══════════════════════════════════════════════════════════════════════════════
# IDENTITY & SECURITY
# ═══════════════════════════════════════════════════════════════════════════════
serviceAccount:
  create: true
  name: "tavana"
  annotations:
    # Azure Workload Identity
    azure.workload.identity/client-id: ""

podLabels:
  azure.workload.identity/use: "true"

# Pod Security (Azure Policy compliant)
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault

containerSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL

# ═══════════════════════════════════════════════════════════════════════════════
# NETWORKING
# ═══════════════════════════════════════════════════════════════════════════════
networkPolicies:
  enabled: true
  egressCIDRs:
    - 10.0.0.0/8
    - 100.64.0.0/10
    - 168.63.0.0/16

# Ingress via Azure Application Gateway
ingress:
  enabled: true
  className: "azure/application-gateway"
  annotations:
    appgw.ingress.kubernetes.io/ssl-redirect: "true"
    appgw.ingress.kubernetes.io/connection-draining: "true"
    appgw.ingress.kubernetes.io/request-timeout: "300"
    appgw.ingress.kubernetes.io/backend-protocol: "http"
    appgw.ingress.kubernetes.io/health-probe-path: "/health"
  hosts:
    - host: ""  # REQUIRED
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: tavana-tls
      hosts:
        - ""  # REQUIRED

# ═══════════════════════════════════════════════════════════════════════════════
# OBSERVABILITY
# ═══════════════════════════════════════════════════════════════════════════════
monitoring:
  prometheus:
    enabled: false

config:
  logLevel: "info"

rbac:
  create: true

podDisruptionBudget:
  gateway:
    enabled: true
    minAvailable: 1
  worker:
    enabled: true
    maxUnavailable: 1
