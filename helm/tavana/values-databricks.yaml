# Tavana Helm Chart Values - Databricks Integration
# 
# This configuration enables Tavana to:
# 1. Query Delta Lake tables from Databricks-managed storage (ADLS Gen2, S3)
# 2. Accept connections from BI tools that also connect to Databricks
# 3. Provide PostgreSQL-compatible interface for Databricks data

# Global settings
global:
  namespace: tavana
  imageRegistry: "ghcr.io/angelerator"
  imagePullPolicy: IfNotPresent
  imageTag: "latest"

# Gateway settings
gateway:
  enabled: true
  replicaCount: 2
  
  # TLS enabled for production (matches Databricks security)
  tls:
    enabled: true
    selfSigned: false
    existingSecret: "tavana-tls"
    existingSecretCertKey: "tls.crt"
    existingSecretKeyKey: "tls.key"
  
  resources:
    requests:
      memory: "2Gi"
      cpu: "1000m"
    limits:
      memory: "8Gi"
      cpu: "4000m"

# Worker settings with Delta Lake support
worker:
  enabled: true
  replicaCount: 4
  minReplicas: 2
  maxReplicas: 20
  
  resources:
    requests:
      memory: "4Gi"
      cpu: "1000m"
    limits:
      memory: "16Gi"
      cpu: "4000m"
  
  # Environment variables for Databricks/Azure storage access
  env:
    # =====================================================
    # OPTION 1: Azure ADLS Gen2 (Databricks on Azure)
    # =====================================================
    # Storage account name from your Databricks workspace
    - name: AZURE_STORAGE_ACCOUNT_NAME
      value: "yourstorageaccount"
    
    # Use one of these authentication methods:
    
    # A) Managed Identity (recommended for AKS with Workload Identity)
    - name: AZURE_USE_MANAGED_IDENTITY
      value: "true"
    
    # B) SAS Token (for time-limited access)
    # - name: AZURE_SAS_TOKEN
    #   valueFrom:
    #     secretKeyRef:
    #       name: azure-storage-credentials
    #       key: sas-token
    
    # C) Connection String (for development)
    # - name: AZURE_STORAGE_CONNECTION_STRING
    #   valueFrom:
    #     secretKeyRef:
    #       name: azure-storage-credentials
    #       key: connection-string
    
    # =====================================================
    # OPTION 2: AWS S3 (Databricks on AWS)
    # =====================================================
    # - name: AWS_REGION
    #   value: "us-east-1"
    # - name: AWS_ACCESS_KEY_ID
    #   valueFrom:
    #     secretKeyRef:
    #       name: aws-credentials
    #       key: access-key-id
    # - name: AWS_SECRET_ACCESS_KEY
    #   valueFrom:
    #     secretKeyRef:
    #       name: aws-credentials
    #       key: secret-access-key
    
    # =====================================================
    # DuckDB Configuration for large Delta tables
    # =====================================================
    - name: DUCKDB_TEMP_DIR
      value: "/tmp/duckdb"
    - name: DUCKDB_MAX_TEMP_SIZE
      value: "200GB"

# HPA for auto-scaling
hpa:
  enabled: true
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75

# Object storage configuration
objectStorage:
  # Azure ADLS Gen2 (for Databricks on Azure)
  azure:
    enabled: true
    accountName: "yourstorageaccount"
    # Container where Delta tables are stored
    container: "databricks-data"
    # Use managed identity
    useManagedIdentity: true
  
  # AWS S3 (for Databricks on AWS)
  # s3:
  #   enabled: true
  #   region: "us-east-1"
  #   bucket: "databricks-data"

# =============================================================================
# USAGE EXAMPLES
# =============================================================================
#
# After deploying with these values, connect via PostgreSQL protocol:
#
# 1. Connect with psql:
#    psql "host=tavana-gateway.tavana.svc port=5432 user=tavana password=tavana sslmode=require"
#
# 2. Query Delta Lake tables from Databricks storage:
#
#    -- Azure ADLS Gen2 (abfss:// protocol)
#    SELECT * FROM delta_scan('abfss://databricks-data@yourstorageaccount.dfs.core.windows.net/silver/customers')
#    LIMIT 100;
#
#    -- AWS S3
#    SELECT * FROM delta_scan('s3://databricks-data/silver/customers')
#    LIMIT 100;
#
# 3. Join Databricks Delta tables:
#    SELECT 
#      c.customer_name,
#      SUM(o.total_amount) as lifetime_value
#    FROM delta_scan('abfss://data@storage.dfs.core.windows.net/silver/customers') c
#    JOIN delta_scan('abfss://data@storage.dfs.core.windows.net/silver/orders') o
#      ON c.customer_id = o.customer_id
#    GROUP BY c.customer_name
#    ORDER BY lifetime_value DESC
#    LIMIT 10;
#
# 4. Connect from Python (same libraries as Databricks):
#
#    import psycopg2
#    
#    conn = psycopg2.connect(
#        host="tavana-gateway.example.com",
#        port=5432,
#        user="tavana",
#        password="tavana",
#        sslmode="require"
#    )
#    
#    cursor = conn.cursor()
#    cursor.execute("""
#        SELECT * FROM delta_scan('abfss://container@account.dfs.core.windows.net/table')
#    """)
#    results = cursor.fetchall()
#
# 5. Connect from Tableau/Power BI:
#    - Use PostgreSQL connector
#    - Server: tavana-gateway.example.com
#    - Port: 5432
#    - Database: tavana
#    - Username: tavana
#    - SSL: Required
#
# =============================================================================


