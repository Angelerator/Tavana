# ═══════════════════════════════════════════════════════════════════════════════
# Tenant-Dedicated Worker Pool Template
# ═══════════════════════════════════════════════════════════════════════════════
#
# This template is used by TenantPoolManager to dynamically create
# dedicated worker pools for each tenant. You can also manually deploy
# tenant pools using this template.
#
# Usage:
#   TENANT_ID=company-abc ./deploy-tenant.sh
# Or:
#   sed 's/TENANT_ID/company-abc/g' tenant-pool-template.yaml | kubectl apply -f -
#
# Architecture:
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │                    TENANT-DEDICATED WORKER POOL                             │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │                                                                             │
# │  ┌─────────────────────────────────────────────────────────────────────┐   │
# │  │                    TENANT RESOURCES                                  │   │
# │  │                                                                      │   │
# │  │  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐              │   │
# │  │  │ Deployment  │    │   Service   │    │    HPA      │              │   │
# │  │  │ worker-tid  │───▶│ worker-tid  │    │ worker-tid  │              │   │
# │  │  │             │    │             │    │   1-20      │              │   │
# │  │  │ VPA enabled │    │ Port 50053  │    │ CPU/Memory  │              │   │
# │  │  └─────────────┘    └─────────────┘    └─────────────┘              │   │
# │  │                                                                      │   │
# │  │  Benefits:                                                           │   │
# │  │  ✅ Complete data isolation between tenants                         │   │
# │  │  ✅ Per-tenant caching (DuckDB in-memory cache)                     │   │
# │  │  ✅ Per-tenant HPA scaling (1-20 workers per tenant)                │   │
# │  │  ✅ Per-tenant VPA sizing (256MB to 100GB per worker)               │   │
# │  │  ✅ No startup delay (workers always warm)                          │   │
# │  │                                                                      │   │
# │  └──────────────────────────────────────────────────────────────────────┘   │
# │                                                                             │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# Note: The TenantPoolManager creates these dynamically when:
#   - A tenant first submits a large query (>1GB)
#   - An admin explicitly creates a tenant pool
# ═══════════════════════════════════════════════════════════════════════════════

---
# Deployment for tenant-dedicated workers
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker-TENANT_ID
  namespace: tavana
  labels:
    app: tavana-worker-TENANT_ID
    tavana.io/component: worker
    tavana.io/tenant-pool: "true"
    tavana.io/tenant-id: TENANT_ID
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tavana-worker-TENANT_ID
  template:
    metadata:
      labels:
        app: tavana-worker-TENANT_ID
        tavana.io/tenant-pool: "true"
        tavana.io/tenant-id: TENANT_ID
    spec:
      serviceAccountName: tavana-worker
      terminationGracePeriodSeconds: 600  # 10 min for query completion
      containers:
      - name: worker
        image: tavana-worker:duckdb14
        imagePullPolicy: Never
        ports:
        - containerPort: 50053
          name: grpc
          protocol: TCP
        env:
        - name: TENANT_ID
          value: "TENANT_ID"
        - name: LOG_LEVEL
          value: "info"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            # 100GB max per worker for large queries
            memory: "100Gi"
            cpu: "16"
        # Health probes
        startupProbe:
          grpc:
            port: 50053
          failureThreshold: 30
          periodSeconds: 10
        readinessProbe:
          grpc:
            port: 50053
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          grpc:
            port: 50053
          initialDelaySeconds: 30
          periodSeconds: 10
        # Graceful shutdown
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 30"]
      # Enable in-place resize (K8s v1.35)
      enableServiceLinks: false

---
# Service for tenant-dedicated workers
apiVersion: v1
kind: Service
metadata:
  name: worker-TENANT_ID
  namespace: tavana
  labels:
    tavana.io/tenant-pool: "true"
    tavana.io/tenant-id: TENANT_ID
spec:
  selector:
    app: tavana-worker-TENANT_ID
  ports:
  - port: 50053
    targetPort: 50053
    protocol: TCP
    name: grpc
  type: ClusterIP

---
# HPA for tenant-dedicated workers
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-TENANT_ID-hpa
  namespace: tavana
  labels:
    tavana.io/tenant-pool: "true"
    tavana.io/tenant-id: TENANT_ID
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: worker-TENANT_ID
  # Per-tenant scaling limits
  minReplicas: 1
  maxReplicas: 20
  metrics:
  # Scale up on memory pressure
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # Scale up on CPU pressure
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    # Scale up quickly when needed
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 5
        periodSeconds: 15
      selectPolicy: Max
    # Scale down slowly to preserve cache
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 min stabilization
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60  # 1 pod per minute
      selectPolicy: Min

---
# PodDisruptionBudget for tenant workers
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: worker-TENANT_ID-pdb
  namespace: tavana
  labels:
    tavana.io/tenant-pool: "true"
    tavana.io/tenant-id: TENANT_ID
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: tavana-worker-TENANT_ID

---
# NetworkPolicy for tenant isolation (optional - for strict isolation)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: worker-TENANT_ID-isolation
  namespace: tavana
  labels:
    tavana.io/tenant-pool: "true"
    tavana.io/tenant-id: TENANT_ID
spec:
  podSelector:
    matchLabels:
      app: tavana-worker-TENANT_ID
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Only allow gateway to connect
  - from:
    - podSelector:
        matchLabels:
          app: tavana-gateway
    ports:
    - protocol: TCP
      port: 50053
  egress:
  # Allow S3/external data sources
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 9000  # MinIO
  # Allow DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    ports:
    - protocol: UDP
      port: 53

