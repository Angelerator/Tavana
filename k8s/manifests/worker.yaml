# Tavana Worker Deployment
# Uses K8s v1.35 features: in-place resize, restart rules
# Managed by HPA (count) and VPA (size)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker
  namespace: tavana
  labels:
    app: tavana
    component: worker
spec:
  # Initial replicas - HPA takes over immediately
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0  # Never reduce capacity during rollout
      maxSurge: 1
  selector:
    matchLabels:
      app: tavana-worker
  template:
    metadata:
      labels:
        app: tavana-worker
    spec:
      # Graceful shutdown - allow queries to complete
      terminationGracePeriodSeconds: 60
      containers:
        - name: worker
          image: tavana-worker:latest
          imagePullPolicy: Never
          ports:
            - containerPort: 50053
          env:
            # Connection pool size per worker
            - name: POOL_SIZE
              value: "4"
            - name: LOG_LEVEL
              value: "info"
            # DuckDB settings - will be adjusted by VPA
            - name: MAX_MEMORY_GB
              value: "0"  # 0 = auto-detect from container limits
            - name: DUCKDB_TEMP_DIR
              value: "/tmp/duckdb"
            - name: DUCKDB_MAX_TEMP_SIZE
              value: "50GB"
          envFrom:
            - configMapRef:
                name: tavana-config
          # Start small - VPA will resize as needed
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "400Gi"  # Max allowed by VPA
              cpu: "64"
          # K8s v1.35: In-place resize policy
          # Allows VPA to resize without restarting the pod
          resizePolicy:
            - resourceName: memory
              restartPolicy: NotRequired
            - resourceName: cpu
              restartPolicy: NotRequired
          # Startup probe: allow DuckDB to initialize
          startupProbe:
            tcpSocket:
              port: 50053
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 12
          # Readiness probe: worker can accept queries
          readinessProbe:
            tcpSocket:
              port: 50053
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          # Liveness probe: worker is responsive
          livenessProbe:
            tcpSocket:
              port: 50053
            initialDelaySeconds: 15
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          volumeMounts:
            - name: duckdb-temp
              mountPath: /tmp/duckdb
      volumes:
        - name: duckdb-temp
          emptyDir:
            sizeLimit: 50Gi
---
apiVersion: v1
kind: Service
metadata:
  name: worker
  namespace: tavana
spec:
  selector:
    app: tavana-worker
  ports:
    - port: 50053
      targetPort: 50053
  # K8s v1.35: Prefer routing to workers on the same node
  # Reduces network latency for query execution
  trafficDistribution: PreferSameNode
---
# PodDisruptionBudget: ensure at least 1 worker available
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: worker-pdb
  namespace: tavana
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: tavana-worker
