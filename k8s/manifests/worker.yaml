# Tavana Worker Deployment
# Uses K8s v1.35 features: in-place resize, restart rules
# Managed by HPA (count) and VPA (size)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker
  namespace: tavana
  labels:
    app: tavana
    component: worker
spec:
  # Initial replicas - HPA takes over immediately
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0  # Never reduce capacity during rollout
      maxSurge: 1
  selector:
    matchLabels:
      app: tavana-worker
  template:
    metadata:
      labels:
        app: tavana-worker
    spec:
      # Graceful shutdown - allow long queries to complete (10 min max)
      terminationGracePeriodSeconds: 600
      containers:
        - name: worker
          image: tavana-worker:latest
          imagePullPolicy: Never
          ports:
            - containerPort: 50053
          # PreStop hook: wait for in-flight queries to complete
          # Worker stops accepting new queries, drains existing ones
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    echo "Graceful shutdown: waiting for queries to complete..."
                    # Give worker time to finish current queries
                    # In production, check for active queries via health endpoint
                    sleep 30
                    echo "Shutdown complete"
          env:
            # Connection pool size per worker
            - name: POOL_SIZE
              value: "4"
            - name: LOG_LEVEL
              value: "info"
            # DuckDB settings - will be adjusted by VPA
            - name: MAX_MEMORY_GB
              value: "0"  # 0 = auto-detect from container limits
            - name: DUCKDB_TEMP_DIR
              value: "/tmp/duckdb"
            - name: DUCKDB_MAX_TEMP_SIZE
              value: "50GB"
          envFrom:
            - configMapRef:
                name: tavana-config
          # Start small - VPA will resize as needed
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "400Gi"  # Max allowed by VPA
              cpu: "64"
          # K8s v1.35: In-place resize policy
          # Allows VPA to resize without restarting the pod
          resizePolicy:
            - resourceName: memory
              restartPolicy: NotRequired
            - resourceName: cpu
              restartPolicy: NotRequired
          # Startup probe: allow DuckDB to initialize
          startupProbe:
            tcpSocket:
              port: 50053
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 12
          # Readiness probe: worker can accept queries
          readinessProbe:
            tcpSocket:
              port: 50053
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          # Liveness probe: worker is responsive
          livenessProbe:
            tcpSocket:
              port: 50053
            initialDelaySeconds: 15
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          volumeMounts:
            - name: duckdb-temp
              mountPath: /tmp/duckdb
      volumes:
        - name: duckdb-temp
          emptyDir:
            sizeLimit: 50Gi
---
apiVersion: v1
kind: Service
metadata:
  name: worker
  namespace: tavana
spec:
  selector:
    app: tavana-worker
  ports:
    - port: 50053
      targetPort: 50053
  # K8s v1.35: Prefer routing to workers on the same node
  # Reduces network latency for query execution
  trafficDistribution: PreferSameNode
---
# PodDisruptionBudget: ensure at least 1 worker available
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: worker-pdb
  namespace: tavana
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: tavana-worker
---
# CronJob: Enforce 5-hour max lifetime for workers
# This recycles old workers to clear DuckDB cache and prevent memory bloat
apiVersion: batch/v1
kind: CronJob
metadata:
  name: worker-lifecycle-manager
  namespace: tavana
spec:
  # Run every 30 minutes
  schedule: "*/30 * * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: gateway-sa
          restartPolicy: OnFailure
          containers:
          - name: lifecycle-manager
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              echo "=== Worker Lifecycle Manager ==="
              echo "Max lifetime: 5 hours (18000 seconds)"
              echo "Checking for old workers..."
              
              # Get current time
              NOW=$(date +%s)
              MAX_AGE=18000  # 5 hours in seconds
              
              # Get all worker pods with their creation times
              kubectl get pods -n tavana -l app=tavana-worker -o json | \
              jq -r '.items[] | select(.status.phase=="Running") | 
                "\(.metadata.name) \(.metadata.creationTimestamp)"' | \
              while read POD_NAME CREATED_AT; do
                # Convert creation time to epoch
                POD_AGE_EPOCH=$(date -d "$CREATED_AT" +%s 2>/dev/null || date -j -f "%Y-%m-%dT%H:%M:%SZ" "$CREATED_AT" +%s 2>/dev/null)
                AGE=$((NOW - POD_AGE_EPOCH))
                AGE_HOURS=$((AGE / 3600))
                
                echo "Pod: $POD_NAME, Age: ${AGE_HOURS}h (${AGE}s)"
                
                if [ $AGE -gt $MAX_AGE ]; then
                  echo ">>> Evicting $POD_NAME (older than 5 hours)"
                  kubectl delete pod -n tavana "$POD_NAME" --grace-period=60
                fi
              done
              
              echo "=== Lifecycle check complete ==="
