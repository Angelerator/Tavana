# Worker HPA - DISABLED (SmartScaler controls scaling)
# 
# ARCHITECTURE:
# SmartScaler handles BOTH:
#   - HPA (pod count): Adaptive formula-based scaling
#   - VPA (pod size): Pre-sizing + Elastic resize
#
# K8s HPA is disabled by setting targets to 99% (never triggers)
# This prevents conflicts between K8s HPA and SmartScaler

---
# PASSIVE HPA - only provides bounds, never triggers scaling
# All scaling decisions are made by SmartScaler's adaptive formula
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-hpa
  namespace: tavana
  annotations:
    description: "PASSIVE - SmartScaler controls scaling, not this HPA"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: worker
  minReplicas: 2
  maxReplicas: 100
  metrics:
  # Set to 99% - will never trigger (SmartScaler handles scaling)
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 99
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 99
  behavior:
    # SCALE UP: Effectively disabled - very slow (SmartScaler handles this)
    scaleUp:
      stabilizationWindowSeconds: 1800  # 30 min - effectively disabled
      policies:
      - type: Pods
        value: 1  # Minimum allowed
        periodSeconds: 1800  # Maximum allowed
      selectPolicy: Min
    # SCALE DOWN: Effectively disabled - very slow (SmartScaler handles this)
    scaleDown:
      stabilizationWindowSeconds: 1800  # 30 min - effectively disabled
      policies:
      - type: Pods
        value: 1  # Minimum allowed
        periodSeconds: 1800  # Maximum allowed
      selectPolicy: Min
---
# Option 2: KEDA ScaledObject (recommended for production)
# Uses custom metrics: active_queries, queue_depth
# 
# To use: Install KEDA first:
#   helm repo add kedacore https://kedacore.github.io/charts
#   helm install keda kedacore/keda --namespace keda --create-namespace
#
# Then apply this ScaledObject (uncomment below):

# apiVersion: keda.sh/v1alpha1
# kind: ScaledObject
# metadata:
#   name: worker-scaledobject
#   namespace: tavana
# spec:
#   scaleTargetRef:
#     name: worker
#   # Allow scaling to zero during idle periods (cost savings!)
#   minReplicaCount: 0
#   maxReplicaCount: 100
#   # 5 min cooldown before scaling down
#   cooldownPeriod: 300
#   # Advanced settings
#   advanced:
#     horizontalPodAutoscalerConfig:
#       behavior:
#         scaleUp:
#           stabilizationWindowSeconds: 0
#           policies:
#           - type: Percent
#             value: 100
#             periodSeconds: 15
#         scaleDown:
#           stabilizationWindowSeconds: 300
#           policies:
#           - type: Pods
#             value: 1
#             periodSeconds: 120
#   triggers:
#   # PRIMARY: Scale on active queries (most accurate!)
#   - type: prometheus
#     metadata:
#       serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
#       metricName: tavana_active_queries
#       # Scale when avg queries per worker > 3
#       threshold: "3"
#       query: |
#         sum(tavana_active_queries) / 
#         clamp_min(count(kube_pod_info{namespace="tavana", pod=~"worker.*"}), 1)
#   # SECONDARY: Scale on queue depth (handle bursts)
#   - type: prometheus
#     metadata:
#       serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
#       metricName: tavana_query_queue_depth
#       # Scale when queue > 5 queries
#       threshold: "5"
#       query: tavana_query_queue_depth
#   # TERTIARY: Scale on query rate (predictive scaling)
#   - type: prometheus
#     metadata:
#       serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
#       metricName: tavana_query_rate
#       # Scale when rate > 10 queries/sec
#       threshold: "10"
#       query: tavana_query_rate
